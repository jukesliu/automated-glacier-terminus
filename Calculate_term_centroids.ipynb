{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to calculate the centroids of the (trimmed) terminus picks\n",
    "\n",
    "#### Jukes Liu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pl\n",
    "import matplotlib.image as mpimg\n",
    "import datetime\n",
    "import types\n",
    "import os\n",
    "import cv2\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If output images have not yet been converted to png format from pgm, do it using cell magic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "cd /media/jukes/jukes1/LS8aws/Box033/rotated/resized/\n",
    "mogrify -format png *.pgm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Set up\n",
    "\n",
    "- set BoxIDs to calculate centroids for\n",
    "- set mass or size\n",
    "- define the centroid function\n",
    "- read in dates from datetags.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BoxIDs = ['001', '002', '004', '033', '120', '174', '235', '259', '277', '531']\n",
    "massorsize = \"mass\"\n",
    "\n",
    "#Define the centroid function\n",
    "def centroid(x, y):\n",
    "    length = len(x)\n",
    "    return sum(x) / length, sum(y) / length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1687, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Img_Date</th>\n",
       "      <th>datetimes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LC80360042017077LGN00</td>\n",
       "      <td>2017-03-18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LC80360042015248LGN00</td>\n",
       "      <td>2015-09-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LC80360042015184LGN00</td>\n",
       "      <td>2015-07-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LC80360042016107LGN00</td>\n",
       "      <td>2016-04-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LC80360042015232LGN00</td>\n",
       "      <td>2015-08-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Img_Date   datetimes\n",
       "0  LC80360042017077LGN00  2017-03-18\n",
       "1  LC80360042015248LGN00  2015-09-05\n",
       "2  LC80360042015184LGN00  2015-07-03\n",
       "3  LC80360042016107LGN00  2016-04-16\n",
       "4  LC80360042015232LGN00  2015-08-20"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read in datetags csv as datetime_df\n",
    "datetime_df = pd.read_csv('/home/jukes/Documents/Sample_glaciers/datetags.csv', sep=',', dtype=str, header=0, names=['Img_Date', 'datetimes'])\n",
    "print(datetime_df.shape)\n",
    "datetime_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) OPTION A: Calculate centroids for terminus picked using a metric (mass or size)\n",
    "\n",
    "Grabs the terminus pick line coordinates from the .dat files generated from the 2D WTMM in Xsmurf and calculates their centroid using the centroid function. This calculates the centroids for the original dat file (term_dat) AND the trimmed terminus pick (term_trim_dat). Currently only outputs the trimmed termini centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box001\n",
      "137 137 137 137 137\n",
      "Box002\n",
      "29 29 29 29 29\n",
      "Box004\n",
      "20 20 20 20 20\n",
      "Box033\n",
      "94 94 94 94 94\n",
      "Box120\n",
      "106 106 106 106 106\n",
      "Box174\n",
      "47 47 47 47 47\n",
      "Box235\n",
      "93 93 93 93 93\n",
      "Box259\n",
      "76 76 76 76 76\n",
      "Box277\n",
      "62 62 62 62 62\n",
      "Box531\n",
      "208 208 208 208 208\n"
     ]
    }
   ],
   "source": [
    "centroid_xs = []\n",
    "centroid_ys = []\n",
    "BOIs_final = []\n",
    "scenenames = []\n",
    "basepath = '/media/jukes/jukes1/LS8aws/'\n",
    "metric = \"terminus_highest\"+massorsize+\"/\" \n",
    "\n",
    "for BOI in BoxIDs:\n",
    "    print(\"Box\"+BOI)\n",
    "    imagepath = basepath+\"Box\"+BOI+\"/rotated/resized/\"\n",
    "\n",
    "#     #make results directory in BoxID folder if it doesn't already exist\n",
    "#     if os.path.exists(basepath+\"Box\"+BOI+\"/Results/\"):\n",
    "#         print(\"RESULTS FOLDER EXISTS ALREADY. SKIP.\")\n",
    "#     #OTHERWISE, create the folder and download into it\n",
    "#     else:\n",
    "#         os.mkdir(basepath+\"Box\"+BOI+\"/Results/\")\n",
    "#         print(\"Results  folder made\")\n",
    "\n",
    "    #make lists to store image data and grab image files\n",
    "    imgfiles = os.listdir(imagepath)\n",
    "    image_arrays = []\n",
    "    dats = []\n",
    "    trimdats = []\n",
    "    imgnames = []\n",
    "    avgpix_values = []\n",
    "    skews = []\n",
    "    BOIs =[]\n",
    "\n",
    "    for imgfile in imgfiles:\n",
    "        #grab image files and append to images list\n",
    "        if imgfile.endswith(BOI+\".png\"):\n",
    "    #         print(imgfile)\n",
    "            image = mpimg.imread(imagepath+imgfile)\n",
    "            imgname = imgfile[0:-4]\n",
    "            scenename = imgname[7:-16]\n",
    "            pathtodat = imagepath+imgname+\".pgm_max_gaussian/\"+metric\n",
    "            datfiles = os.listdir(pathtodat)\n",
    "            \n",
    "        \n",
    "            #NOT FILTERING FOR CLOUDS AGAIN CURRENTLY:\n",
    "            #If pixel values are skewed toward 1, it's prob cloudy\n",
    "            pixelvals = image.reshape(image.shape[0]*image.shape[1])\n",
    "    #         print(pixelvals.shape)\n",
    "            skew = scipy.stats.skew(pixelvals, bias=False)\n",
    "\n",
    "            avgpix_val = np.average(pixelvals)\n",
    "            avgpix_thresh = 0.50  \n",
    "\n",
    "            #if there are 2 datfiles and not cloudy, grab the trimmed and non-trimmed files\n",
    "            if len(datfiles) == 2: #and avgpix_val < avgpix_thresh and skew > -0.80:\n",
    "                #append the image array and the image name to the list\n",
    "                image_arrays.append(image)\n",
    "                imgnames.append(scenename)\n",
    "                skews.append(skew)\n",
    "                avgpix_values.append(avgpix_val)\n",
    "                BOIs.append(BOI)\n",
    "\n",
    "                #find the trimmed dat file and the original\n",
    "                for dat in datfiles:\n",
    "                    if \"trim\" in dat:\n",
    "                        datfile_trim = dat\n",
    "                        trimdats.append(datfile_trim)\n",
    "                    else:\n",
    "                        datfile = dat\n",
    "                        dats.append(datfile)\n",
    "\n",
    "    #         print(image, datfile_trim, datfile)\n",
    "#             else:\n",
    "#                 print(\"NO DAT FILES CREATED FOR TERMINUS PICK\")\n",
    "\n",
    "    print(len(image_arrays), len(dats), len(trimdats), len(imgnames), len(avgpix_values))\n",
    "    images_df = pd.DataFrame(list(zip(imgnames, BOIs, image_arrays, dats, trimdats, avgpix_values, skews)),\n",
    "                  columns=['Scene', 'BoxID','Image array', 'Dat file name', \"Trimmed dat file name\", 'Avg pix val', 'Skew'])\n",
    "    \n",
    "    #JOIN DATAFRAMES\n",
    "    images_df.sort_values(by='Scene')\n",
    "    # images_df\n",
    "    datetime_df = datetime_df.sort_values(by='Img_Date')\n",
    "#     print(datetime_df.head())\n",
    "    \n",
    "    new_df = images_df.set_index('Scene').join(datetime_df.set_index('Img_Date'))\n",
    "    dated_images_df = new_df.sort_values(by='datetimes')\n",
    "#     print(dated_images_df.head())\n",
    "\n",
    "    #CALCULATE ALL CENTROIDS\n",
    "    for index, row in dated_images_df.iterrows():\n",
    "        imagename = index\n",
    "        trimdat = row['Trimmed dat file name']\n",
    "        dat = row['Dat file name']\n",
    "        BoxID = row['BoxID']\n",
    "        \n",
    "        datpath = basepath+\"Box\"+BoxID+\"/rotated/resized/crop_R_\"+imagename+\"_B8_PS_Buffer\"+BoxID+\".pgm_max_gaussian/\"+metric\n",
    "\n",
    "        #Read in dat file as np array and grab x and y values\n",
    "        #TRIMMED:\n",
    "        term_trim_dat = np.loadtxt(datpath+trimdat)\n",
    "\n",
    "        #ORIGINAL:\n",
    "        term_dat = np.loadtxt(datpath+dat)\n",
    "\n",
    "        #ORIGINAL\n",
    "        term_xs = []\n",
    "        term_ys = []\n",
    "\n",
    "        #grab x and y values for the terminus line\n",
    "        for j in term_dat:\n",
    "            x, y = (j[0], j[1])\n",
    "            term_xs.append(x)\n",
    "            term_ys.append(y)\n",
    "\n",
    "        #TRIMMED\n",
    "        term_trim_xs = []\n",
    "        term_trim_ys = []\n",
    "\n",
    "        #grab x and y values for the terminus line\n",
    "        for j in term_trim_dat:\n",
    "    #         print(j)\n",
    "            x, y = (j[0], j[1])\n",
    "            term_trim_xs.append(x)\n",
    "            term_trim_ys.append(y)\n",
    "\n",
    "        #CALCULATE CENTROIDS AND APPEND TO LISTS\n",
    "        center_x, center_y = centroid(term_xs, term_ys)\n",
    "        trim_center_x, trim_center_y = centroid(term_trim_xs, term_trim_ys)\n",
    "        centroid_xs.append(trim_center_x)\n",
    "        centroid_ys.append(trim_center_y)\n",
    "        BOIs_final.append(BoxID)\n",
    "        scenenames.append(imagename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) OPTION B: Calculate midpoints for terminus picked using a metric (mass or size)\n",
    "\n",
    "Grabs the terminus pick line coordinates from the .dat files generated from the 2D WTMM in Xsmurf and identifies the middle one/ This calculates the centroids for the trimmed terminus pick (term_trim_dat). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box001\n",
      "164 164 164 164 164\n",
      "Box002\n",
      "29 29 29 29 29\n",
      "Box004\n",
      "20 20 20 20 20\n",
      "Box033\n",
      "112 112 112 112 112\n",
      "Box120\n",
      "103 103 103 103 103\n",
      "Box174\n",
      "60 60 60 60 60\n",
      "Box235\n",
      "124 124 124 124 124\n",
      "Box259\n",
      "115 115 115 115 115\n",
      "Box277\n",
      "62 62 62 62 62\n",
      "Box531\n",
      "237 237 237 237 237\n"
     ]
    }
   ],
   "source": [
    "midpoints_xs = []\n",
    "midpoints_ys = []\n",
    "BOIs_final = []\n",
    "scenenames = []\n",
    "basepath = '/media/jukes/jukes1/LS8aws/'\n",
    "metric = \"terminus_highest\"+massorsize+\"/\" \n",
    "\n",
    "for BOI in BoxIDs:\n",
    "    print(\"Box\"+BOI)\n",
    "    imagepath = basepath+\"Box\"+BOI+\"/rotated/resized/\"\n",
    "\n",
    "#     #make results directory in BoxID folder if it doesn't already exist\n",
    "#     if os.path.exists(basepath+\"Box\"+BOI+\"/Results/\"):\n",
    "#         print(\"RESULTS FOLDER EXISTS ALREADY. SKIP.\")\n",
    "#     #OTHERWISE, create the folder and download into it\n",
    "#     else:\n",
    "#         os.mkdir(basepath+\"Box\"+BOI+\"/Results/\")\n",
    "#         print(\"Results  folder made\")\n",
    "\n",
    "    #make lists to store image data and grab image files\n",
    "    imgfiles = os.listdir(imagepath)\n",
    "    image_arrays = []\n",
    "    dats = []\n",
    "    trimdats = []\n",
    "    imgnames = []\n",
    "    avgpix_values = []\n",
    "    skews = []\n",
    "    BOIs =[]\n",
    "\n",
    "    for imgfile in imgfiles:\n",
    "        #grab image files and append to images list\n",
    "        if imgfile.endswith(BOI+\".png\"):\n",
    "    #         print(imgfile)\n",
    "            image = mpimg.imread(imagepath+imgfile)\n",
    "            imgname = imgfile[0:-4]\n",
    "            scenename = imgname[7:-16]\n",
    "            pathtodat = imagepath+imgname+\".pgm_max_gaussian/\"+metric\n",
    "            datfiles = os.listdir(pathtodat)\n",
    "            \n",
    "        \n",
    "            #NOT FILTERING FOR CLOUDS AGAIN CURRENTLY:\n",
    "            #If pixel values are skewed toward 1, it's prob cloudy\n",
    "            pixelvals = image.reshape(image.shape[0]*image.shape[1])\n",
    "    #         print(pixelvals.shape)\n",
    "            skew = scipy.stats.skew(pixelvals, bias=False)\n",
    "\n",
    "            avgpix_val = np.average(pixelvals)\n",
    "            avgpix_thresh = 0.50  \n",
    "\n",
    "            #if there are 2 datfiles and not cloudy, grab the trimmed and non-trimmed files\n",
    "            if len(datfiles) == 2: #and avgpix_val < avgpix_thresh and skew > -0.80:\n",
    "                #append the image array and the image name to the list\n",
    "                image_arrays.append(image)\n",
    "                imgnames.append(scenename)\n",
    "                skews.append(skew)\n",
    "                avgpix_values.append(avgpix_val)\n",
    "                BOIs.append(BOI)\n",
    "\n",
    "                #find the trimmed dat file and the original\n",
    "                for dat in datfiles:\n",
    "                    if \"trim\" in dat:\n",
    "                        datfile_trim = dat\n",
    "                        trimdats.append(datfile_trim)\n",
    "                    else:\n",
    "                        datfile = dat\n",
    "                        dats.append(datfile)\n",
    "\n",
    "    #         print(image, datfile_trim, datfile)\n",
    "#             else:\n",
    "#                 print(\"NO DAT FILES CREATED FOR TERMINUS PICK\")\n",
    "\n",
    "    print(len(image_arrays), len(dats), len(trimdats), len(imgnames), len(avgpix_values))\n",
    "    images_df = pd.DataFrame(list(zip(imgnames, BOIs, image_arrays, dats, trimdats, avgpix_values, skews)),\n",
    "                  columns=['Scene', 'BoxID','Image array', 'Dat file name', \"Trimmed dat file name\", 'Avg pix val', 'Skew'])\n",
    "    \n",
    "    #JOIN DATAFRAMES\n",
    "    images_df.sort_values(by='Scene')\n",
    "    # images_df\n",
    "    datetime_df = datetime_df.sort_values(by='Img_Date')\n",
    "#     print(datetime_df.head())\n",
    "    \n",
    "    new_df = images_df.set_index('Scene').join(datetime_df.set_index('Img_Date'))\n",
    "    dated_images_df = new_df.sort_values(by='datetimes')\n",
    "#     print(dated_images_df.head())\n",
    "\n",
    "    #CALCULATE ALL MIDPOINTS\n",
    "    for index, row in dated_images_df.iterrows():\n",
    "        imagename = index\n",
    "        trimdat = row['Trimmed dat file name']\n",
    "        dat = row['Dat file name']\n",
    "        BoxID = row['BoxID']\n",
    "        \n",
    "        datpath = basepath+\"Box\"+BoxID+\"/rotated/resized/crop_R_\"+imagename+\"_B8_PS_Buffer\"+BoxID+\".pgm_max_gaussian/\"+metric\n",
    "\n",
    "        #Read in dat file as np array and grab x and y values\n",
    "        #TRIMMED:\n",
    "        term_trim_dat = np.loadtxt(datpath+trimdat)\n",
    "\n",
    "        #TRIMMED\n",
    "        term_trim_xs = []\n",
    "        term_trim_ys = []\n",
    "\n",
    "        #grab x and y values for the terminus line\n",
    "        for j in term_trim_dat:\n",
    "    #         print(j)\n",
    "            x, y = (j[0], j[1])\n",
    "            term_trim_xs.append(x)\n",
    "            term_trim_ys.append(y)\n",
    "        \n",
    "        #determine the index of the \"midpoint\" of the terminus line to find the x and y coordinate of it\n",
    "        mid_index = int(np.round_(len(term_trim_xs)/2))\n",
    "        trim_mid_x = term_trim_xs[mid_index]\n",
    "        trim_mid_y = term_trim_ys[mid_index]\n",
    "\n",
    "        #APPEND TO LISTS       \n",
    "        midpoints_xs.append(trim_mid_x)\n",
    "        midpoints_ys.append(trim_mid_y)\n",
    "        BOIs_final.append(BoxID)\n",
    "        scenenames.append(imagename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Store the centroids in a DataFrame and export to a csv file\n",
    "\n",
    "\n",
    "Exports the trimmed terminus midpoints to a csv file called __trim_term_midpoints.csv__.\n",
    "\n",
    "OR \n",
    "\n",
    "Exports the trimmed terminus centroids to a csv file called __trim_centroids.csv__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scene</th>\n",
       "      <th>BoxID</th>\n",
       "      <th>Mid_X</th>\n",
       "      <th>Mid_Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LC80330052013125LGN01</td>\n",
       "      <td>001</td>\n",
       "      <td>146.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LC80320052013134LGN03</td>\n",
       "      <td>001</td>\n",
       "      <td>169.0</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LC80330052013141LGN01</td>\n",
       "      <td>001</td>\n",
       "      <td>184.0</td>\n",
       "      <td>153.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LC80360042013146LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>145.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LC80340052013148LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>145.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LC80310052013239LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>140.0</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LC80340052013244LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>143.0</td>\n",
       "      <td>162.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LC80350052013251LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>139.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LC80360042013258LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>158.0</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>LC80340052013260LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>138.0</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>LC80330052013269LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>135.0</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>LC80310052013271LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>138.0</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LC80320052014073LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>158.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LC80350052014078LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>155.0</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LC80310052014082LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>153.0</td>\n",
       "      <td>160.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LC80340052014087LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>152.0</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LC80310052014098LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>148.0</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LC80360042014101LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>148.0</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LC80340052014103LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>148.0</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LC80350052014126LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>146.0</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LC80340052014135LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>151.0</td>\n",
       "      <td>176.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LC80370042014140LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>155.0</td>\n",
       "      <td>161.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LC80350052014142LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>158.0</td>\n",
       "      <td>163.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LC80330052014144LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>147.0</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>LC80310052014146LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>146.0</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>LC80340052014151LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>120.0</td>\n",
       "      <td>167.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>LC80320052014153LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>146.0</td>\n",
       "      <td>159.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>LC80370042014156LGN01</td>\n",
       "      <td>001</td>\n",
       "      <td>169.0</td>\n",
       "      <td>158.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>LC80330052014160LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>181.0</td>\n",
       "      <td>156.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>LC80340052014167LGN00</td>\n",
       "      <td>001</td>\n",
       "      <td>148.0</td>\n",
       "      <td>166.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>LC80130022016250LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>232.0</td>\n",
       "      <td>257.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>LC80110022016252LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>236.0</td>\n",
       "      <td>229.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>LC80080022016263LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>224.0</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>LC80150012016264LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>189.0</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>LC80060032016265LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>241.0</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>LC80130022016266LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>238.0</td>\n",
       "      <td>238.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>LC80110022016268LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>233.0</td>\n",
       "      <td>257.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>LC80070022016272LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>235.0</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>LC80060032017075LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>180.0</td>\n",
       "      <td>256.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>LC80110022017078LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>172.0</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>LC80160012017081LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>233.0</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>LC80100022017087LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>173.0</td>\n",
       "      <td>251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>LC80080022017089LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>171.0</td>\n",
       "      <td>249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>LC80150012017090LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>197.0</td>\n",
       "      <td>240.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>LC80060032017091LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>201.0</td>\n",
       "      <td>287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>LC80110022017094LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>173.0</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>LC80090022017096LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>241.0</td>\n",
       "      <td>245.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>LC80070032017098LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>202.0</td>\n",
       "      <td>288.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1014</th>\n",
       "      <td>LC80140012017099LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>230.0</td>\n",
       "      <td>259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1015</th>\n",
       "      <td>LC80120022017101LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>231.0</td>\n",
       "      <td>260.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1016</th>\n",
       "      <td>LC80100022017103LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>173.0</td>\n",
       "      <td>250.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1017</th>\n",
       "      <td>LC80080022017105LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>172.0</td>\n",
       "      <td>249.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1018</th>\n",
       "      <td>LC80060032017107LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>202.0</td>\n",
       "      <td>287.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>LC80130022017108LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>226.0</td>\n",
       "      <td>259.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1020</th>\n",
       "      <td>LC80110022017110LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>227.0</td>\n",
       "      <td>255.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>LC80090022017112LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>173.0</td>\n",
       "      <td>251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1022</th>\n",
       "      <td>LC80160012017113LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>241.0</td>\n",
       "      <td>251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>LC80070032017114LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>173.0</td>\n",
       "      <td>251.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1024</th>\n",
       "      <td>LC80140012017115LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>241.0</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1025</th>\n",
       "      <td>LC80100022017119LGN00</td>\n",
       "      <td>531</td>\n",
       "      <td>241.0</td>\n",
       "      <td>248.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1026 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Scene BoxID  Mid_X  Mid_Y\n",
       "0     LC80330052013125LGN01   001  146.0  160.0\n",
       "1     LC80320052013134LGN03   001  169.0  149.0\n",
       "2     LC80330052013141LGN01   001  184.0  153.0\n",
       "3     LC80360042013146LGN00   001  145.0  160.0\n",
       "4     LC80340052013148LGN00   001  145.0  160.0\n",
       "5     LC80310052013239LGN00   001  140.0  158.0\n",
       "6     LC80340052013244LGN00   001  143.0  162.0\n",
       "7     LC80350052013251LGN00   001  139.0  160.0\n",
       "8     LC80360042013258LGN00   001  158.0  161.0\n",
       "9     LC80340052013260LGN00   001  138.0  159.0\n",
       "10    LC80330052013269LGN00   001  135.0  154.0\n",
       "11    LC80310052013271LGN00   001  138.0  163.0\n",
       "12    LC80320052014073LGN00   001  158.0  160.0\n",
       "13    LC80350052014078LGN00   001  155.0  161.0\n",
       "14    LC80310052014082LGN00   001  153.0  160.0\n",
       "15    LC80340052014087LGN00   001  152.0  158.0\n",
       "16    LC80310052014098LGN00   001  148.0  159.0\n",
       "17    LC80360042014101LGN00   001  148.0  158.0\n",
       "18    LC80340052014103LGN00   001  148.0  158.0\n",
       "19    LC80350052014126LGN00   001  146.0  158.0\n",
       "20    LC80340052014135LGN00   001  151.0  176.0\n",
       "21    LC80370042014140LGN00   001  155.0  161.0\n",
       "22    LC80350052014142LGN00   001  158.0  163.0\n",
       "23    LC80330052014144LGN00   001  147.0  159.0\n",
       "24    LC80310052014146LGN00   001  146.0  159.0\n",
       "25    LC80340052014151LGN00   001  120.0  167.0\n",
       "26    LC80320052014153LGN00   001  146.0  159.0\n",
       "27    LC80370042014156LGN01   001  169.0  158.0\n",
       "28    LC80330052014160LGN00   001  181.0  156.0\n",
       "29    LC80340052014167LGN00   001  148.0  166.0\n",
       "...                     ...   ...    ...    ...\n",
       "996   LC80130022016250LGN00   531  232.0  257.0\n",
       "997   LC80110022016252LGN00   531  236.0  229.0\n",
       "998   LC80080022016263LGN00   531  224.0  248.0\n",
       "999   LC80150012016264LGN00   531  189.0  248.0\n",
       "1000  LC80060032016265LGN00   531  241.0  239.0\n",
       "1001  LC80130022016266LGN00   531  238.0  238.0\n",
       "1002  LC80110022016268LGN00   531  233.0  257.0\n",
       "1003  LC80070022016272LGN00   531  235.0  240.0\n",
       "1004  LC80060032017075LGN00   531  180.0  256.0\n",
       "1005  LC80110022017078LGN00   531  172.0  250.0\n",
       "1006  LC80160012017081LGN00   531  233.0  239.0\n",
       "1007  LC80100022017087LGN00   531  173.0  251.0\n",
       "1008  LC80080022017089LGN00   531  171.0  249.0\n",
       "1009  LC80150012017090LGN00   531  197.0  240.0\n",
       "1010  LC80060032017091LGN00   531  201.0  287.0\n",
       "1011  LC80110022017094LGN00   531  173.0  250.0\n",
       "1012  LC80090022017096LGN00   531  241.0  245.0\n",
       "1013  LC80070032017098LGN00   531  202.0  288.0\n",
       "1014  LC80140012017099LGN00   531  230.0  259.0\n",
       "1015  LC80120022017101LGN00   531  231.0  260.0\n",
       "1016  LC80100022017103LGN00   531  173.0  250.0\n",
       "1017  LC80080022017105LGN00   531  172.0  249.0\n",
       "1018  LC80060032017107LGN00   531  202.0  287.0\n",
       "1019  LC80130022017108LGN00   531  226.0  259.0\n",
       "1020  LC80110022017110LGN00   531  227.0  255.0\n",
       "1021  LC80090022017112LGN00   531  173.0  251.0\n",
       "1022  LC80160012017113LGN00   531  241.0  251.0\n",
       "1023  LC80070032017114LGN00   531  173.0  251.0\n",
       "1024  LC80140012017115LGN00   531  241.0  252.0\n",
       "1025  LC80100022017119LGN00   531  241.0  248.0\n",
       "\n",
       "[1026 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midpoints_df = pd.DataFrame(list(zip(scenenames, BOIs_final, midpoints_xs, midpoints_ys)),\n",
    "              columns=['Scene','BoxID', 'Mid_X','Mid_Y'])\n",
    "\n",
    "#save as\n",
    "midpoints_df.to_csv(path_or_buf = '/home/jukes/Documents/Sample_glaciers/trim_term_midpoints_'+massorsize+'.csv', sep=',')\n",
    "midpoints_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# centroids_df = pd.DataFrame(list(zip(scenenames, BOIs_final, centroid_xs, centroid_ys)),\n",
    "#               columns=['Scene','BoxID', 'Centroid_X','Centroid_Y'])\n",
    "\n",
    "# #save as\n",
    "# centroids_df.to_csv(path_or_buf = '/home/jukes/Documents/Sample_glaciers/trim_centroids_'+massorsize+'.csv', sep=',')\n",
    "# centroids_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
